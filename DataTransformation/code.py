#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
This file is automatically generated by AION for AION_449_1 usecase.
File generation time: 2022-07-05 13:40:48
'''
#Standard Library modules
import platform
import time
import logging
import shutil
import json
import sys
import argparse

#Third Party modules
from pathlib import Path
import pandas as pd 
from sklearn.preprocessing import LabelEncoder
import joblib
from sklearn.impute import SimpleImputer

input_file = {
    "inputData": "rawData.dat",
    "metaData": "modelMetaData.json"
}
output_file = {
    "log": "aion.log",
    "metaData": "modelMetaData.json",
    "outputData": "transformedData.dat",
    "targetEncoder": "targetEncoder.pkl"
}
                    
def read_json(file_path):                    
    data = None                    
    with open(file_path,'r') as f:                    
        data = json.load(f)                    
    return data                    
                    
def write_json(data, file_path):                    
    with open(file_path,'w') as f:                    
        json.dump(data, f)                    
                    
def read_data(file_path, encoding='utf-8', sep=','):                    
    return pd.read_csv(file_path, encoding=encoding, sep=sep)                    
                    
def write_data(data, file_path, index=False):                    
    return data.to_csv(file_path, index=index)                    
                    
#Uncomment and change below code for google storage                    
#def write_data(data, file_path, index=False):                    
#    file_name= file_path.name                    
#    data.to_csv('output_data.csv')                    
#    storage_client = storage.Client()                    
#    bucket = storage_client.bucket('aion_data')                    
#    bucket.blob('prediction/'+file_name).upload_from_filename('output_data.csv', content_type='text/csv')                    
#    return data                    
                    
def is_file_name_url(file_name):                    
    supported_urls_starts_with = ('gs://','https://','http://')                    
    return file_name.startswith(supported_urls_starts_with)                    

                    
log = None                    
def set_logger(log_file, mode='a'):                    
    global log                    
    logging.basicConfig(filename=log_file, filemode=mode, format='%(asctime)s %(name)s- %(message)s', level=logging.INFO, datefmt='%d-%b-%y %H:%M:%S')                    
    log = logging.getLogger(Path(__file__).parent.name)                    
    return log                    
                    
def get_logger():                    
    return log

                    
def add_file_for_production(meta_data, file):                    
    if 'prod_files' not in meta_data.keys():                    
        meta_data['prod_files'] = []                    
    meta_data['prod_files'].append(file)                    
                    
def copy_prod_files(source, target, meta_data):                    
    if 'prod_files' in meta_data.keys():                    
        for file in meta_data['prod_files']:                    
            if not (target/file).exists():                    
                if (source/file).exists():                    
                    shutil.copy(source/file, target/file)
                    
def log_dataframe(df, msg=None):                    
    import io                    
    buffer = io.StringIO()                    
    df.info(buf=buffer)                    
    if msg:                    
        log_text = f'Data frame after {msg}:'                    
    else:                    
        log_text = 'Data frame:'                    
    log_text += '\n\t'+str(df.head(2)).replace('\n','\n\t')                    
    log_text += ('\n\t' + buffer.getvalue().replace('\n','\n\t'))                    
    get_logger().info(log_text)
        
def validateConfig(base_config):        
    config_file = Path(__file__).parent/'config.json'        
    if not Path(config_file).exists():        
        raise ValueError(f'Config file is missing: {config_file}')        
    config = read_json(config_file)        
    if not base_config['inputPath']:        
        base_config['inputPath'] = config['inputPath']        
        
    if not base_config['outputPath']:        
        base_config['outputPath'] = config['outputPath']        
        
    if not Path(base_config['inputPath']).exists():        
        if not is_file_name_url(base_config['inputPath']):        
            loc = base_config['inputPath']        
            raise ValueError(f'Data location does not exists: {loc}')        
    config = read_json(config_file)        
    return base_config,config


def transformation(base_config):        
    base_config,config = validateConfig(base_config)        
    inputPath = Path(base_config['inputPath'])        
    outputPath = Path(base_config['outputPath'])        
    outputPath.mkdir(parents=True, exist_ok=True)        
    if not outputPath.exists():        
        raise ValueError(f'Target Path not found at {outputPath}')        
    meta_data_file = inputPath/input_file['metaData']        
    if meta_data_file.exists():        
        meta_data = read_json(meta_data_file)        
    else:        
        raise ValueError(f'Configuration file not found: {meta_data_file}')        
    log_file = outputPath/output_file['log']        
    logger = set_logger(log_file)        
    dataLoc = inputPath/input_file['inputData']        
    if not dataLoc.exists():        
        return {'Status':'Failure','Message':'Data location does not exists.'}        
    add_file_for_production(meta_data, input_file['inputData'])        
    status = dict()        
    df = read_data(dataLoc)        
    log_dataframe(df)        
        
    target_feature = config['target_feature']        
    train_features = [x for x in config['train_features'] if x != target_feature]        
    num_features = [x for x in config['num_features'] if x != target_feature]        
        
    df = df[[target_feature] + train_features]        
    meta_data['transformation'] = {}        
    df = df.dropna(axis=0, subset=[target_feature])        
    df = df.dropna(axis=0, how='all', subset=df.columns)        
    df = df.drop_duplicates(keep='first')        
    df = df.reset_index(drop=True)
    target_encoder = LabelEncoder()
    df[config['target_feature']] = target_encoder.fit_transform(df[config['target_feature']])
    target_encoder_file_name = str(outputPath/output_file['targetEncoder'])
    add_file_for_production(meta_data, output_file['targetEncoder'])
    joblib.dump(target_encoder, target_encoder_file_name)
    meta_data['transformation']['target_encoder'] = output_file['targetEncoder']
    logger.info('Categorical to numeric conversion done for target feature')
    meta_data['transformation']['fillna'] = {}
    cat_features = [x for x in config['cat_features'] if x != target_feature]
    cat_imputer = SimpleImputer(strategy='most_frequent')
    df[cat_features + config['cat_num_features']] = cat_imputer.fit_transform(df[cat_features + config['cat_num_features']])
    for index, col in enumerate(cat_features + config['cat_num_features']):
        meta_data['transformation']['fillna'][col] = cat_imputer.statistics_[index]
    logger.info(f'Missing values replaced for categorical columns {cat_features}')
    num_imputer = SimpleImputer(strategy='median')
    df[num_features] = num_imputer.fit_transform(df[num_features])
    for index, col in enumerate(num_features):
        meta_data['transformation']['fillna'][col] = num_imputer.statistics_[index]
    logger.info(f'Missing values replaced for numeric columns {num_features}')                
    log_dataframe(df)                
    csv_path = str(outputPath/output_file['outputData'])                
    write_data(df, csv_path,index=False)                
    status = {'Status':'Success','DataFilePath':output_file['outputData'], 'text_profiler':{}}                
    meta_data['transformation']['Status'] = status                
    add_file_for_production(meta_data, output_file['metaData'])                
    write_json(meta_data, str(outputPath/output_file['metaData']))                
    copy_prod_files(inputPath, outputPath, meta_data)                
    logger.info(f'Transformed data saved at {csv_path}')                
    logger.info(f'output: {status}')                
    return json.dumps(status)
        
if __name__ == '__main__':        
    parser = argparse.ArgumentParser()        
    parser.add_argument('-i', '--inputPath', help='path of the input data')        
    parser.add_argument('-o', '--outputPath', help='path for saving the output data')        
    args = parser.parse_args()        
        
    config = {'inputPath':None,'outputPath':None}        
        
    if args.inputPath:        
        config['inputPath'] = args.inputPath        
    if args.outputPath:        
        config['outputPath'] = args.outputPath        
        
    try:        
        print(transformation(config))        
    except Exception as e:        
        get_logger().error(e, exc_info=True)        
        status = {'Status':'Failure','Message':str(e)}        
        print(json.dumps(status))