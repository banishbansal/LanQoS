#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
This file is automatically generated by AION for AION_449_1 usecase.
File generation time: 2022-07-05 13:40:49
'''
#Standard Library modules
import logging
import sys
import json
import time
import platform
import tempfile
import shutil
import argparse
import sklearn
import pandas as pd
from pathlib import Path
#Third Party modules
from pathlib import Path

input_file = {
"performance": "performance.json",
"actualData":"actual.csv",
"predictData":"prediction.csv" 
}

def read_json(file_path):                    
    data = None                    
    with open(file_path,'r') as f:                    
        data = json.load(f)                    
    return data                    

def is_drift_within_limits(train_matrices, current_matrices, threshold = 5):
    scoring_criteria = train_matrices['scoring_criteria']
    train_score = train_matrices['metrices']['test_score']/100
    current_score = current_matrices[scoring_criteria]
    threshold_value = train_score * threshold / 100.0
    if current_score > (train_score - threshold_value) :
        return True
    else:
        return False

def get_metrices(actual_values, predicted_values):        
    from sklearn.metrics import accuracy_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import f1_score
    result = {} 
    accuracy_score = accuracy_score(actual_values, predicted_values)        
    avg_precision = precision_score(actual_values, predicted_values,        
        average='macro')        
    avg_recall = recall_score(actual_values, predicted_values,        
        average='macro')        
    avg_f1 = f1_score(actual_values, predicted_values,        
        average='macro')        
        
    result['accuracy'] = accuracy_score        
    result['precision'] = avg_precision        
    result['recall'] = avg_recall        
    result['f1'] = avg_f1        
    return result            
            	
def monitoring(config):
    from input_drift import inputdrift
    output_json = {}
    historicaldataFrame=pd.read_csv(config['rawDataInputPath'])        
    currentdataFrame=pd.read_csv(config['newDataInputPath']) 	
    inputdriftObj = inputdrift(config)
    dataalertcount,inputdrift_message = inputdriftObj.get_input_drift(currentdataFrame,historicaldataFrame)	
    actual_data_path = Path(config['predictDataPath']/input_file['actualData'])
    predict_data_path = Path(config['predictDataPath']/input_file['predictData'])
    performance= Path(config['predictDataPath']/input_file['performance'])
    if performance.exists():        
        performance = read_json(performance)  
        if actual_data_path.exists() and predict_data_path.exists():
            predicted_data = pd.read_csv(predict_data_path)        		
            actual_data_path = pd.read_csv(actual_data_path)
            mergedRes = pd.merge(actual_data_path, predicted_data, on ='AION_ID',how = 'inner')
            currentPerformance = {} 			
            currentPerformance = get_metrices(mergedRes['actual'], mergedRes['prediction'])
            if is_drift_within_limits(performance, currentPerformance):
                output_json.update({'outputDrift':'Model score is with in limits'})
            else:
                output_json.update({'outputDrift':{'Meassage': 'Model's output is drifted','trainedScore':performance['metrices']['test_score'], 'currentScore':currentPerformance[performance['scoring_criteria']]}})
    else:        
        raise ValueError(f'Configuration file not found: {meta_data_file}')	
    if inputdrift_message == 'Model is working as expected':        
        output_json.update({'status':'SUCCESS','inputDrift':{'Message':'Model is working as expected'}})        
    else:        
        output_json.update({'status':'SUCCESS','inputDrift':{'Affected Columns':inputdrift_message}})        
    return(output_json)        
	
if __name__ == '__main__':        
    parser = argparse.ArgumentParser()        
    parser.add_argument('-r', '--rawDataInputPath', help='Training Data Path')
    parser.add_argument('-n', '--newDataInputPath', help='New Data Path')	
    parser.add_argument('-i', '--predictDataPath', help='Production Folder Path')
    parser.add_argument('-o','--outputPath',help='Output Folder Path')
    args = parser.parse_args()        
        
    config = {'rawDataInputPath':None,'inputUri':None,'newDataInputPath':None,'inputPath':None,'outputPath':None,'ml':False}        
        
    if args.rawDataInputPath:        
        config['rawDataInputPath'] = Path(args.rawDataInputPath )        
    if args.newDataInputPath:        
        config['newDataInputPath'] = Path(args.newDataInputPath )
    if args.predictDataPath:        
        config['predictDataPath'] = Path(args.predictDataPath )
    if args.outputPath:        
        config['outputPath'] = Path(args.outputPath )		
        
    try:        
        print(monitoring(config))        
    except Exception as e:        
        status = {'Status':'Failure','Message':str(e)}        
        print(json.dumps(status))
